# Chapter 01 - 初识 MCP 及 MCP 工作原理

> 听完 Ch01，你对 MCP 有了什么新认识？欢迎你分享。

对话即生产，模型即生产要素！
对话即生产，模型即生产要素！
对话即生产，模型即生产要素！

MCP（模型上下文协议）的出现扩展并深化了对话式生产方式的边界，使得大模型不仅能思考，还能执行具体的操作。

MCP 得到了各大科技公司和 AI 公司的支持，包括苹果公司和 OpenAI，显示了其在 AI 领域的重要性。

```
2022年11月 - ChatGPT 3.5 发布
2023年11月 - GPT Store 发布
2024年11月 - Anthropic 发布 MCP 协议
2025年3月  - OpenAI 支持 MCP
2025年4月  - 谷歌支持 MCP
```

## 1. 什么是 MCP？

MCP（模型上下文协议）是由 Anthropic 推出的开放标准，其最核心的价值是充当大语言模型（LLM）与外部世界（如数据库、API、文件系统）进行安全、标准化交互的"通用桥梁"。

### 1.1 技术架构

MCP 由三个核心组件构成：

- 主机：支持 MCP 的 AI 软件，负责调用大模型并创建与 MCP 客户端的连接。
- MCP 客户端：位于主机内部，负责与 MCP 服务器进行通信，传递主机的请求和接收服务器的响应。
- MCP 服务器：执行具体的操作，可以分为本地服务器和远程服务器。

MCP 就像**电脑的 USB 接口**：
- 主机 = 电脑主机（AI 大模型）
- MCP = 统一接口标准
- 外设 = 各种工具（打印机、摄像头、3D 打印机、移动硬盘、NAS 等）

通过统一的接口，可以连接众多外设，提升 AI 大模型的功能边界。

### 1.2 传输机制

​传输机制的选择依据​：选择哪种通信方式（stdio 或 SSE）取决于具体的部署场景。

- ​stdio（标准输入/输出）​​：通常在客户端和服务器位于同一台计算机上时使用。例如，在个人电脑上，Claude Desktop 应用程序会以子进程的形式启动一个本地的 MCP 服务器脚本，并通过标准输入输出流进行通信。这种方式延迟低，但局限于本地。
- ​SSE（服务器发送事件）​​：用于跨网络的远程通信。它允许服务器通过一个长连接主动向客户端推送消息（如实时更新），而客户端则通过 HTTP POST 请求向服务器发送指令。这适用于分布式部署和需要实时更新的场景。
- ​演进：Streamable HTTP​：值得注意的是，MCP 的协议本身也在演进。根据较新的技术资料，Anthropic 引入了一种称为“Streamable HTTP”的新传输方式，旨在替代原有的 SSE 方式，进一步简化服务器实现，并更好地支持无状态和云原生（Serverless）部署。

用户向主机提出请求 -> ​主机将请求和可用工具列表提供给大模型（LLM）​​ -> ​LLM 决策并生成工具调用指令 -> ​主机通过其内部的客户端将指令发送给服务器​ -> ​服务器执行任务并返回结果给客户端 -> 结果最终经由主机展示给用户

#### JSON-RPC 2.0 格式

JSON-RPC 2.0 是一种轻量级且无状态的远程过程调用协议，其核心在于定义了结构清晰、易于理解的 JSON 消息格式，以确保通信双方能准确交互。其消息主要分为请求和响应两种类型。

|消息类型 | 核心成员 | 是否必须 | 描述|
|请求 (Request)|jsonrpc|必须 | 字符串，必须为 "2.0"，用于标识协议版本。|
| |method|必须 | 字符串，要调用的远程方法名。|
| |params|可选 | 可以是数组或对象，包含要传递给方法的参数。如果方法无需参数，可省略此成员。|
| |id|条件必须 | 请求的唯一标识符，可以是字符串、数字或 null。用于匹配响应。若请求为“通知”，则必须不包含此成员。|
|响应 (Response)|jsonrpc|必须 | 字符串，必须为 "2.0"。|
| |result|成功时必须 | 调用成功时返回的结果，由被调用的方法决定其值。不能与 error 同时出现。|
| |error|失败时必须 | 调用失败时返回的错误信息，是一个对象。不能与 result 同时出现。|
| |id|必须 | 与对应请求中的 id 值相同（通知请求无响应，故无此 id）。|

JSON-RPC 2.0 支持批量请求，客户端可以将一个包含多个请求对象的数组作为单个请求发送。服务器会处理所有请求，并返回一个包含对应响应对象的数组（通知请求没有响应项）。这有助于减少网络开销。

### 1.3 MCP 能力

- 工具 (Tools)：模型的“双手”
工具是模型能够主动执行的函数或操作，通常会影响外部环境。例如，一个“创建日历事件”工具，模型在理解你的意图后，可以调用它来直接操作你的日历应用。关键在于，调用工具通常需要模型的主动决策，某些敏感操作可能还需要得到用户的明确许可。这就像是模型有了“双手”，可以去实际操作世界。
- 资源 (Resources)：模型的“参考资料”
资源为模型提供了丰富的只读数据源。模型可以像查阅资料一样读取它们，但不会对其进行修改。例如，通过 MCP 服务器连接公司内部的知识库，模型就能在回答问题时引用最新的产品文档。这极大地扩展了模型的知识边界，使其能基于实时、准确的信息进行回答，而不是仅仅依赖训练时的旧数据。
- 提示 (Prompts)：模型的“专业顾问”
提示是由 MCP 服务器提供的预置提示模板，旨在引导模型在特定场景下做出更专业、更符合预期的行为。比如，一个专为代码审查优化的 MCP 服务器，会提供一个结构化的“代码审查提示”，当模型使用这个提示时，它会更聚焦于发现代码中的安全性、性能等问题。这相当于为模型请了一位“专业顾问”，让它在不同领域都能发挥出专家水平。

在实际应用中，这三者常常是协同工作的。想象一个智能客服场景：
1. 模型首先查询资源（公司知识库），了解最新的退货政策。
2. 接着，它可能会使用一个提示（标准客服话术模板），来组织友好且专业的回复语言。
3. 最后，在获得用户确认后，模型调用工具（订单系统 API），为用户执行实际的退货操作。

## 2. 为什么需要 MCP？

### 2.1 MCP 解决的核心问题

**在 MCP 之前的困境：**
- 各个 AI 应用定义自己的规范
- 假设使用了 10 个 Agent，每个由不同的人创建
- 当某个 Agent 接口变化时，需要相应调整所有依赖
- 开发者需要适配多种不同的接口标准

**MCP 带来的改变：**
- 统一了调用外部资源的方式
- 开发者不必关心用户的具体指令
- 只需满足 MCP 服务器的统一格式即可
- 简化了开发流程和用户使用流程

### 2.2 MCP 的核心价值

**1. 标准化**
- 提供统一的接口标准
- 大模型通过一个接口连接任何工具

**2. 动态工具集**
- 可以灵活添加和使用各种工具
- 扩展大模型的能力边界

**3. 不中断对话（最重要）**
- 用户与大模型的对话无需中断
- 自动调用第三方工具
- 维持沟通的自然和流畅
- 避免频繁切换界面
- 提升生产效率的关键

### 2.3 MCP 的局限性

**安全性问题：**
- MCP 使用 JSON-RPC，安全性相对较弱
- 需要注意数据安全和隐私保护
 

## 3. MCP 的安装配置

### 3.1 安装配置三步骤

**第一步：安装 AI 软件（MCP 主机）**
- 选择支持 MCP 的软件平台
- 如：Claude、Trae、Cherry Studio 等
- 负责调用大模型并创建 MCP 连接

**第二步：安装前置软件（运行环境）**

MCP 服务器本质上是一段代码，需要特定环境才能执行：

**运行环境包括：**
- **Node.js** 解释器 + **npm** 包管理器
- **Python** 解释器 + **uv** 包管理器

**第三步：配置 MCP 服务器参数（启动命令）**

**核心命令：**
- **uvx**：uv 提供的下载命令（Python 生态）
- **npx**：Node.js 提供的命令

uvx（Python 生态）和 npx（Node.js 生态）的核心特点是：自动安装（无需预先全局安装，直接运行命令即可临时获取工具）、环境隔离（uvx 为每次运行创建临时虚拟环境；npx 将工具下载到临时目录执行，避免污染全局或项目环境）以及自动销毁（工具执行完毕后，临时环境或文件会被自动清理）。两者都旨在实现工具的即用即弃，保证环境洁净。uvx 基于 Rust 编写，速度通常更快；npx 则随 Node.js 分发，在 JavaScript 生态中应用广泛。


## 4. MCP 资源推荐

### 4.1 官方资源

- [MCP 源](https://www.mcp.bar/)
- [ModelScope MCP](https://www.modelscope.cn/mcp)
- [Smithery MCP](https://smithery.ai/)
- [MCP 官方规范与文档](https://modelcontextprotocol.io/)

### 4.2 开源项目

- [Cherry Studio MCP](https://github.com/CherryHQ/cherry-studio)
- [MCP Registry](https://github.com/mcp)

